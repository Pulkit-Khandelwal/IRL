\documentclass{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[titletoc,toc,page]{appendix}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{verbatim}
\usepackage{url}
\usepackage{natbib}
\usepackage[colorlinks=true,citecolor=blue]{hyperref}

\newtheorem{theorem}{Theorem}
\newtheorem{property}{Property}

\title{Semi-Supervised Learning on Graphs \\
for Inverse Reinforcement Learning}
\author{Charles Reizine, Ã‰lie Michel}
\date{October 2016}

\begin{document}

\maketitle

\section{Introduction}

\paragraph{FIXME}
The goal of \emph{Inverse Reinforcement Learning} (IRL) is to learn the reward function in a \emph{Markov Decision Process} (MDP) from the demonstrations of expert agents. It has many applications [like?] but, as many ML problems, it often suffers from lack of training data. [cite paper pointing this out like \cite{Vasquez14}]. \emph{Semi-Supervised Learning} (SSL) is a typical approach to tackle this issue. In this work, we explore the application of SSL methods to the class of IRL problems.

Why is it interesting to focus on this?
Which are the challenges?

 - IRL is ill-posed

 - pb of measure of similarity to apply ssl


\paragraph{TODO} To be cited:

\cite{Ng99} -> reward shaping, not really IRL but faces the idea of a not totally known reward

\cite{Abbeel04}

\subsubsection{Introduction to the Reinforcement Learning}

The general idea of Reinforcement Learning (RL) is to determine the optimal action policy of a given agent so that it maximizes a \emph{reward} (also called \emph{reinforcement function}) $R$ provided by its interaction with an environment.

Formalized by \cite{Sutton98} as a policy building for a MDP

A finite MDP is a tuple $(S,A,P_{sa},\gamma,R)$, where
\begin{itemize}
    \item $S$ is a finite set of $N$ \emph{states}.
    \item $A=\{a_1,...,a_k\}$ is a set of $k$ \emph{actions} 
    \item $P_{sa}$ are the state \emph{transition probabilities} upon taking action $a$ in state $s$
    \item $\gamma$ is the \emph{discount factor}
    \item $R$ is the \emph{reinforcement function} (bounded) 
\end{itemize}

RL has been applied to a wide variety of problems ranging from .. to .. [cite] with impressive results, especially in the field of games [cite alpha go]. But in many real life problems, the main bottleneck to the application of RL is that the hypothesis of a perfectly known reward function $R$ is not satisfied. For instance, the exact reward maximized during car driving [cite RL paper for autonomous driving but not IRL] or human walk [cite] remains unknown.

Noticing this issue, \cite{Russell98} introduces the notion of IRL. Instead of looking for a policy given a reward, the algorithm is provided with a policy and asked to determine a reward function for which this policy is optimal. In other words, IRL is intended to build a reward that can explain the observed behavior of an agent.

This problem has remained ill-posed for some time. A first attempt to formalized it is presented in \cite{Ng00}, which especially highlights that the problem misses some constraint for the trivial solution of a constant reward not to be a valid output.

The authors apply IRL to a toy problem (5x5 grid world) but quickly notice that this approach scales up poorly to more complex problems that don't provide a policy. The input is hence relaxed from knowing the whole optimal policy to knowing only a set of trajectories sampled using this trajectories. This matches a setup in which one can record from an expert agent (e.g. a human driver for autonomous driving problem) its sequence of actions given a situation, or many situations, but not explicit the full policy, which is virtually impossible in continuous state spaces\footnote{as soon as the problem does not admit any analytic solution}.


The general idea of Reinforcement Learning (RL) \cite{Sutton98} is to build an optimal policy out of a Markov Decision Process based on a reward (also called \emph{Reinforcement Function}).

The problem is that on complex problems, it's very hard to define a reward function that will actually describe well the problem and that could generate an optimal policy that would fit the expectancies.

Indeed, if we consider the example of autonomous cars \cite{Vasquez14} for example, what kind of Reward function could we expect?

\paragraph{}
Inverse Reinforcement Learning (IRL) has been created to solve such issues. Here the strategy is the opposite one. It has been first introduced by \cite{Russell98} and then formalized by \cite{Ng00}. The idea is to use optimal strategies in a variety of circumstances to build a reward function that could explain the associated policy.

\subsubsection{General process of Inverse Reinforcement Learning and limitations}

For this study, we will base our analysis on \cite{Ng00}.

\paragraph{Notation and problem formulation}

A finite MDP is a tuple $(S,A,P_{sa},\gamma,R)$, where
\begin{itemize}
    \item $S$ is a finite set of $N$ \emph{states}.
    \item $A={a_{1},...,a_{k}}$ is a set of $k$ \emph{actions} 
    \item $P_{sa}$ are the state \emph{transition probabilities} upon taking action $a$ in state $s$
    \item $\gamma$ is the \emph{discount factor}
    \item $R$ is the \emph{reinforcement function} (bounded) 
\end{itemize}

The problem of IRL is to find an efficient way of characterizing a good reward function. A reward function is considered as good if it can explain the observed behavior.

We will take the example of a simple case where the model is known, the complete policy is observed and the state is finite. 

Formally, let's consider that we have a given a finite state space $S$, a set of $k$ actions $A={a_{1},...,a_{k}}$, transition probabilities $P_{sa}$, a discount factor $\gamma$ and a policy $\pi$. We are trying to find the set of reward functions $R$ such that $\pi$ is an optimal policy associated to the MDP  $(S,A,P_{sa},\gamma,R)$.

\paragraph{Characterization of the solution set}

In order to get simplicated notations, we will assume that for each state $\pi( s) = a_{1} $. 
One characterization proved in \cite{Ng00} is the following:

\begin{property}
Given a MDP  $(S,A,P_{sa},\gamma,R)$ where S is a finite space and $\gamma \quad \epsilon \quad (0,1)$. Then the policy $\pi$ given by $ \pi(s)=a_{1}$ is optimal if and only if, for all $a=a_{2},...,a_{k}$, the reward satisfies :

\[
({ P }_{ a_{ 1 } }-{ P }_{ a }){ (I-\gamma { P }_{ { a }_{ 1 } }) }^{ -1 }R\quad \succeq \quad 0
\]
\end{property}

The problem is that for most MDPs, it is very likely that there are many choices of $R$ that would meet the previous criteria (namely a reward function always equal to 0). It becomes therefore important to add criterias to reduce the amount of possible solutions and enhance the chances of finding a reward function that would explain observations.

\paragraph{}
One way of doing so, is to choose $R$ that makes $\pi$ optimal and moreover to foster solutions that behave as close as possible to the policy $\pi$. We will therefore try to maximize :

\[
\sum _{s \in S}\left(Q^\pi(s, a_1) - \max_{a \in A \setminus a_1} Q^\pi(s,a)\right)
\]


\paragraph{IRL from sampled trajectories}

We have previously introduced the concept. Unfortunately, the previous equation assumes that the policy $\pi$ is known. If this policy was actually known, the IRL problem would be quite pointless because its purpose is to find a reward function that can be used to compute an optimal policy.

Considering a state distribution $D$ and an initial state $s_{0}$, the purpose is to find $R$ such that $\pi$ maximizes :
$\mathbb E_{ s_{ 0 }\sim D }\left[ { V }^{ \pi  }({ s }_{ 0 }) \right]$.

In more realistic cases, this policy is unknown and we can have access to it only trough a set of actual trajectories in the state space.

There also is a very strong hypothesis that is made that considers that every trajectories that is given as an input has been generated by the same optimal policy.

In many real life cases, it is very easy to believe that several policies can lead to an optimal behavior.

In most of the applications of the articles, data are in fact generated from an optimal policy.

This leads us to notice that there are strong hypothesis made and strong constrainsts to the IRL process as it requires a large amount of optimal trajectories supposedly generated by the same optimal policy.

\subsection{Combining Graphs and Inverse Reinforcement Learning}

The problem of IRL is that its results are highly dependent on the quality and the quantity of the input trajectories. Indeed, it's easy to understand that the more optimal examples that will be given to the algorithm, the better it will perform.

But as we thought that IRL would solve the problem of the design of the reinforcement function, it just modified the problem.

The problem we were trying to solve was how to design a relevant reward function but now we are facing a new problem. Where to find a large set of optimal strategies while minimizing human intervention and labeling?

In \cite{Ng00}, optimal strategies are generated using the optimal policy. Those optimal strategies are then used to find the reward function and to retrieve the optimal policy but this is a chicken-egg problem.

We decided to focus on the SSL process preceeding the IRL. The purpose of this project would be to associate SSL on trajectories and IRL so that from a small amount of labelled trajectories (bad and good), a large set of optimal examples could be labeled and used as input to the IRL algorithm.

\subsection{Semi-supervised learning in Graphs}


Semi-supervised learning is a class of supervised learning tasks and techniques that also make use of unlabeled data for training. This is useful when a large set of data is available but only a small amount of it has already been labeled. Semi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data).

The use of graphs for SSL has become widely used after : \cite{Zhu03}

It is a transductive approach. 
The idea is to use the input data to build a graphs and to use this graph to label each of the node. \\
To construction of the graph is based on the notion of similarity between nodes.\\ This represents how close two datas are. The closer they are, the more plausible it would be for them to have an identical label.\\
On some simple problems, the similarity can be built from the euclidian distance between the input vectors.\\
Nonetheless, on more complex issues, similarities are harder to build. We will discuss the mater of building a relevant similarity for trajectories in the se part.
When similarities have been computed, the graph can be built and we can proceed to SSL ( via hard or soft harmonic functions for instance ).\\
This is why we will focus on the construction of a relevant similarity function for trajectories.


%%
%% Written by Elie
\subsection{Written by Elie}

\paragraph{TODO}
\emph{Misc though quickly written down by Ã‰lie, to be clarified/reorganized/etc.}

The aim of Inverted Reinforcement Learning (IRL) is to learn $R$ so that it can be used in case the set of actions changes. IRL is not just about \emph{generalizing} $\pi$ but also \emph{porting} it to a different type of agent with an action set $mathcal A' \neq \mathcal A$. %% pas compris du tout ce que tu voulais dire %%

Typically\footnote{Give examples}, the machine has access to different information because it has different sensing and computing capabilities. This is why it is important to recover the \emph{intrinseque} reward and not just mimic the training expert policy.

\paragraph{}
Secondly, IRL can be a way of \textbf{getting control} over an AI. $R$, especially represented as a linear combination of some well-known vectors $\Phi_i$, is a very sparse way of representing the behavior of the agent. It makes it humanly possible to \emph{interprete} (if not \emph{understand}) the behavior of the agent, and eventually to \emph{hack} on it to fit the actual objective better.

\paragraph{}
\emph{To explore:} Beside being a reward function actually used to build an agent, $R$ can also be some kind of \emph{view} on the policy followed by an already trained agent. Picking some very simple $\Phi$ can be a way of comparing two subtargets and hence \emph{read} how an agent, whether it is human or artificial, trades of between these targets.

\paragraph{}
Let's take an extremely simplified example for the sake of illustration. Considering the case of a car driver, and imaginning that we got a way of measuring the subtargets $\Phi_{\text{timing}}$, rewarding for a short origin-to-destination timing, and $\Phi_{\text{security}}$, rewarding for safe driving. Even if there are actually many other subtargets involved in the agent true reward $R$, we will consider a \emph{partial reward} $\hat R$ of the form $\hat R = w_1 \Phi_{\text{security}}$

Contextualisation: DÃ©crire le problÃ¨me et des exemples d'enjeu, d'application. Voiture autonome ?

\paragraph{TODO} Biblio: Donner un petit Ã©tat de l'art du SSL dans les graphs (papiers de Valko) et les articles d'IRL.

Settup/pipeline: graph problem with positive and negative trajectories and then IRL.

\section{Trajectory Similarity}

Important to apply graph methods: measure of similarity.

\paragraph{Question}
How to measure the similarity between two trajectories?

\paragraph{NB} Theoretically, it would be important to take into account the reward when comparing trajectories. For instance, trajectories may be very different but both go through the same set of "important" states (the ones actually shaping the reward). But here we won't consider this because we then aim at performing IRL so we don't know the reward function.

\subsection{Similarities in Multi-Arm Bandits trajectories}

There is no notion of order in Multi-Arm Bandits trajectories, so we consider the historigram of chosen arms, i.e. their distribution.

\subsection{Policy estimation}

This can be generalized to cases in which we have a number of states small wrt. the length of the trajectories. We estimate the underlying matrix $P$ such that:

\[
P_{a,s} = \mathbb P(\text{chosing action } a | \text{being in state } s)
\]

We then compare (KL-divergence?) matrices $P$ estimated from different trajectories to measure their similarity.

\paragraph{Issue} Some states may not be visited by a given trajectory, or visited a too small number of times compared to the number of possible actions. So we introduce a notion of confidence into estimation for each state.

We compare confident enough action distributions and also the vector of confidence among states.

\subsection{Bigger number of states/actions}

How to scale to cases in which the number of choices ($n_S$ states times $n_A$ actions) is big comparend to the length $T$ of the trajectories. Cluster RL states? Learn state representation $\phi$? Wavelets? oO

\section{IRL}

How to use negative trajectories for IRL?

For example, $R^+ - R^-$, but there may be different $R^-$, so we could cluster negative trajectories because there are many different ways of being wrong.

\section{Exemples d'application}

\subsection{MAB}

\emph{Just a theoretical comment.}

\subsection{Student problem}

\subsection{Gridworld}


\section{Limitations}

\subsection{Weak coupling of SSL and IRL}

We just concatenated both steps. No real coupling or collaboration/retroaction.

\subsection{TODO: Other limitations?}

\section{Misc notes}

Many different ways of being wrong: How to learn bad policies



\subsection{A first toy problem}

Started to explore a simple example: pendulum on a cart

using gym

The toy pb we need must have:

\begin{itemize}
    \item A reward function which is basically a trade-off between subtargets that we need to weight
    \item Generating (by traditionnal RL) trajectories with diffÃ©rent parameters for the reward trade-off
    \item Reconstructing as much as possible the original weight of R by IRL
    \item A way of labeling positive and pegative trajectories: equivalent to have a priori estimation of R that is +1 for correct trajectories and -1 for wrong ones. Can we release this to a more general hand-crafted prior R?
\end{itemize}

\bibliographystyle{alpha}
\bibliography{bibliography}

\begin{appendices}
\section{Tasks}

\begin{enumerate}
    \item Port Soft-HFS to Python or Q-learning to MATLAB -> Ã‰lie
    \item Biblio/State-of-the-art intro paragraph about SSL in graphs -> Charles
    \item Biblio/State-of-the-art intro paragraph about IRL -> Charles
    \item MAB theoretical comments (1 paragraph) -> Ã‰lie
    \item generate trajectories for student problem -> Ã‰lie/Charles
    \item apply HFS to student problem -> (w/ and w/o confidence vector, and other similarity variances) -> Ã‰lie/Charles
    \item Discuss about IRL with negative labeling (biblio and test) -> Charles
    \item apply IRL to stduent problem (if enough time)
    \item Gridworld
    \item list limitations -> both
    \item clean up report -> both
\end{enumerate}

\paragraph{Deadline} December 24

\end{appendices}

\end{document}
